from autogen import UserProxyAgent
from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent
from typing import Dict, List, Union, Optional
import logging
from dataclasses import dataclass
import matplotlib.pyplot as plt
from PIL import Image

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class ModelConfig:
    """Configuration for language models and API keys."""
    openai_api_key: str
    
    def get_gpt4_config(self) -> Dict:
        """Returns configuration for GPT-4 model."""
        return {
            "cache_seed": 42,
            "temperature": 0,
            "config_list": [{
                'model': 'gpt-4o',
                'api_key': self.openai_api_key
            }],
            "timeout": 120,
        }

class MultimodalAgent:
    """Handles tasks related to scientific data containing figures."""
    
    def __init__(self, admin_agent: UserProxyAgent, multimodal_agent: MultimodalConversableAgent):
        self.admin = admin_agent
        self.multimodal_agent = multimodal_agent
    
    def multimodal_rag(self, image_path: Optional[str], query: str, context: List[str]) -> str:
        """Gets as an input the human query and the retrieved image or text chunks data from the ChromaDB
           retrieval and provides a comprehensive context aware answer.
           
        Args:
            image_path: Path to the image file or None if no image is available
            query: The user's query to be answered
            context: List of text chunks retrieved from ChromaDB
            
        Returns:
            str: The response generated by the multimodal agent
        """
        try:
            # Display the input image if available
            if image_path:
                try:
                    img = Image.open(image_path)
                    plt.figure(figsize=(10, 8))
                    plt.imshow(img)
                    plt.axis("off")
                    plt.title("Input Image")
                    plt.show()
                except Exception as e:
                    logger.error(f"Error displaying image: {str(e)}")
            
            # Format context as a string
            formatted_context = "\n".join(context) if isinstance(context, list) else str(context)
            
            # Create message for the agent
            message = f"""Answer with details and high precision to the user query: {query}.
            
This is the most relevant retrieved context from ChromaDB: 
{formatted_context}

{"This is the retrieved image <img " + image_path + ">." if image_path else "No image was retrieved."}

Structure your answer based on the retrieved context {"and the image" if image_path else ""}.
Provide a comprehensive answer to the user query.
"""
            
            # Initiate chat with the multimodal agent
            response = self.admin.initiate_chat(
                self.multimodal_agent,
                clear_history=True,
                silent=True,
                max_turns=1,
                message=message,
            )
            
            # Return the agent's response
            return response.chat_history[-1]['content']
            
        except Exception as e:
            logger.error(f"Error in multimodal RAG: {e}")
            return f"Error processing request: {str(e)}"

def create_multimodal_agent(openai_api_key: str, human_input_mode: str = "NEVER") -> MultimodalAgent:
    """
    Create and configure a MultimodalAgent instance.
    
    Args:
        openai_api_key: Your OpenAI API key
        human_input_mode: Input mode for UserProxyAgent ("NEVER" for automation, "ALWAYS" for interactive)
        
    Returns:
        MultimodalAgent: Configured multimodal agent instance
    """
    # Create config
    config = ModelConfig(
        openai_api_key=openai_api_key
    )
    
    # Create agents
    admin_agent = UserProxyAgent(
        name="admin", 
        human_input_mode=human_input_mode,
        code_execution_config=False
    )
    
    multimodal_agent = MultimodalConversableAgent(
        name="multimodal", 
        llm_config=config.get_gpt4_config()
    )
    
    # Create and return multimodal agent
    return MultimodalAgent(
        admin_agent=admin_agent,
        multimodal_agent=multimodal_agent
    )